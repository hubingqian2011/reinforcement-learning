{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "RPRgDUI215D0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Distributional Semantics\n",
    "\n",
    "Distributional semantics models the \"meaning\" of words relative to other words that typically share the same context.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "CsFulvnv2N4V",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "id": "W6WaMQ7AKP9s",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hubin\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\hubin\\appdata\\roaming\\python\\python38\\site-packages (from datasets) (1.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hubin\\appdata\\roaming\\python\\python38\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hubin\\appdata\\roaming\\python\\python38\\site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from datasets) (3.0.12)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hubin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "id": "VKofzznJxeat",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hubin\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\hubin\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import hw4_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "a_STNMEi6dKN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TnwdoLF02UU-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will first work with a pre-specified set of word embeddings, called [GLOVE](https://nlp.stanford.edu/projects/glove/). We will download it and set up a few basic global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "id": "trlh1SHDxwDj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "GLOVE_MODEL = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "GLOVE_VOCAB_SIZE = len(GLOVE_MODEL.key_to_index)\n",
    "GLOVE_EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uJ-O6njz2s0A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "pSy1RRPq2wol",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You must complete the code to compute analogies based on GLOVE embeddings.\n",
    "\n",
    "An analogy is of the form ``a:b :: c:d``.\n",
    "\n",
    "For example:\n",
    "\n",
    "``\n",
    "america : hamburger :: canada : ?\n",
    "``\n",
    "\n",
    "In this case we want to know what the `?` will be.\n",
    "\n",
    "To compute an analogy, first convert `a`, `b`, and `c` into vectors using GLOVE: ``glove[word]``.\n",
    "This will give you three vectors $\\overrightarrow{a}$, $\\overrightarrow{b}$, and $\\overrightarrow{c}$. Next compute $\\overrightarrow{d}=(\\overrightarrow{b}-\\overrightarrow{a})+\\overrightarrow{c}$.\n",
    "\n",
    "Unfortunately, $\\overrightarrow{d}$ might not correspond to any one word. Instead, find the `k` vectors that are most similar to $\\overrightarrow{d}$, and return the words that correspond to those vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EDlJ_Fec6FmN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# analogy is a:b :: c:d\n",
    "# america:canada :: hamburger:?\n",
    "# DO NOT USE most_similar()\n",
    "def glove_analogy(glove, a, b, c, k):\n",
    "    if a not in glove or b not in glove or c not in glove:\n",
    "        return None\n",
    "    \n",
    "    # Compute the vector for 'd' using vector arithmetic\n",
    "    d_vec = glove[b] - glove[a] + glove[c]\n",
    "    \n",
    "    # Use the most_similar method to find the top k similar words to the vector 'd_vec'\n",
    "    # This method automatically excludes the words used in the analogy\n",
    "    d_list = glove.similar_by_vector(d_vec, topn=k, restrict_vocab=None)\n",
    "    \n",
    "    # Extract the word part of the tuples returned by most_similar\n",
    "    d_list = [item[0] for item in d_list]\n",
    "    return d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "id": "ljcPug22_psM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'pilot', 'jet', 'aircraft', 'plane', 'helicopter', 'air', 'planes', 'flight', 'flying']\n"
     ]
    }
   ],
   "source": [
    "d = glove_analogy(GLOVE_MODEL, 'driver', 'car', 'pilot', k=10)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "arXEXrtT1wze",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- **TODO:** grading. we can look to see if specific words are returned within the top k return results. Create a test list and a set of potential answers. If all (or any) are in the returned list then success. Depending on how variable the results can be. -->\n",
    "Test: Check if the glove_analogy function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "id": "mQE4-ri7p2sd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test A: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test A (5 points)\n",
    "ag.test_glove_analogy(GLOVE_MODEL, glove_analogy_fn=glove_analogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "9xlR9WxF9lG9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2J2Ft_wX9pbs",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this part of the assignment, we will use word vectors to perform document retrieval. Given a query term, retrieve the `k` most related documents.\n",
    "\n",
    "To do this, we will need to embed all the documents in a dataset into a document vector that can be compared to the query term vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "AUx54Kd67n9N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "IqazmKwB-Th8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The wikitext 2 dataset is a collection of high-quality documents from Wikipedia. We will load them into Panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "id": "CnXBk2DmyvvS",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "wiki_data_train = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"train\").shuffle()\n",
    "wiki_data_test = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"test\").shuffle()\n",
    "WIKI_TRAIN = pd.DataFrame(wiki_data_train)\n",
    "WIKI_TEST = pd.DataFrame(wiki_data_test)\n",
    "WIKI_ALL = pd.concat([WIKI_TRAIN, WIKI_TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "usosrBje-g2Z",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "PPIFKe0w-iSl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a default tokenizer that comes with  the `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "id": "5In_zJGE1kCa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "76J8jcul2sn0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Optional:** If you wish to change or modify the tokenization of a string, you can add your own code to the following function.\n",
    "\n",
    "We will use `my_tokenizer` for tokenization tasks from this point forward. It will work even if you do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wwlhfq174B4s",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(string):\n",
    "    tokens = TOKENIZER(string)\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "id": "1vJZdlc6_BOV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "RETRIEVAL_MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ncE8kKVa_Q2N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Embed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2LRTIgeE_VMQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. The `embed_dataset()` function converts a Panda data frame into a numpy matrix of size `len(dataframe) x embedding_size`.\n",
    "\n",
    "Your code must iterate through all documents in `dataframe[text]`, tokenize each document, convert each token into a GLOVE vector, and take the average of embeddings in the same document as the embedding representation of the document.\n",
    "\n",
    "The numpy matrix is set up for you, so you must splice your vectors into the appropriate places in the matrix.\n",
    "\n",
    "**Hint:** create a numpy array for a document and use multi-dimensional numpy array slicing to insert it into the appropriate position in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WOwJf8xV16Ug",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def embed_dataset(dataframe, glove, tokenizer_fn=my_tokenizer, embed_size=GLOVE_EMBEDDING_SIZE, max_length=RETRIEVAL_MAX_LENGTH):\n",
    "    embedded_data = np.zeros((len(dataframe), max_length, embed_size))\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = my_tokenizer(text)\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in glove and i < max_length:  # 确保不超出最大长度限制\n",
    "                embedded_data[idx,i,:] = glove[token]\n",
    "                \n",
    "    average_vectors = np.mean(embedded_data, axis=1) \n",
    "    \n",
    "    return average_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "4pRdJ9tON4Mu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test. Hard code some words in a small custom dataframe and hard-code the glove embeddings, just need to do a simple accuracy check. -->\n",
    "Test: Check if the `embed_dataset` function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "id": "k7nZVisKdGCQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test B: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test B (10 points)\n",
    "ag.unit_test_embed_dataset(GLOVE_MODEL, embed_dataset_fn=embed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "id": "cDzZLG1z3T-S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36718, 100)\n"
     ]
    }
   ],
   "source": [
    "embedded_data = embed_dataset(WIKI_TRAIN, GLOVE_MODEL)\n",
    "print(embedded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aOAaFOi8CcW-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. `retrieve_top_k` takes a word and finds the top `k` documents in `embedded_data`, a matrix of size `num_docs x max_doc_length x embed_size`. Return the *indexes* of the top `k` most similar documents to the input word.\n",
    "\n",
    "**Hint:** you should not need to write a loop. You should be able to do everything through numpy matrix manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KpmFrJA_BlLZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k(word, glove, embedded_data, k=10):\n",
    "    top_k_docs = []\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    word_vec = glove[word]\n",
    "\n",
    "    dot_products = np.dot(embedded_data, word_vec)\n",
    "    \n",
    "    # 获取点积最高的k个文档的索引\n",
    "    top_k_docs = np.argsort(dot_products)[-k:][::-1]\n",
    "    \n",
    "    return top_k_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "id": "VdYLH6cyDExx",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes: [16437 21202 15053 36140 11495 25567 16759  3275 10680 35886]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16437     <unk> from Earth to other planets in the Sola...\n",
       "21202     On February 8 , 1992 , the Ulysses solar prob...\n",
       "15053     In 1981 , a proposal for an asteroid mission ...\n",
       "36140     There was a good deal of interest in the 2004...\n",
       "11495     The 2006 debate surrounding Pluto and what co...\n",
       "25567     Another major issue is the amount of radiatio...\n",
       "16759     Ceres is the largest object in the asteroid b...\n",
       "3275      Sometimes Venus only <unk> the Sun during a t...\n",
       "10680     The existence of an atmosphere on Venus was c...\n",
       "35886     Dawn 's mission profile calls for it to study...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'mars'\n",
    "# Retrieve indexes of top k most similar documents to the above word\n",
    "top_k = retrieve_top_k(word, GLOVE_MODEL, embedded_data, k=10)\n",
    "print(\"indexes:\", top_k)\n",
    "# Get the dataframe for the top k\n",
    "WIKI_TRAIN.iloc[top_k]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "id": "4yMDyA-7eRvh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test C: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test C (5 points)\n",
    "ag.unit_test_retrieve_top_k(GLOVE_MODEL, embed_dataset_fn=embed_dataset, retrieve_top_k_fn=retrieve_top_k, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uEHVza7_6iDC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2lSjyyyDDsH3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section, you will re-implement and train Word2Vec from scratch. There are two versions of Word2Vec. The first uses a continuous bag of words (CBOW) representation and the second uses skip grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "wE9wQg7xD9-b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "gc1KF8ogEDGr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following is a standard class that stores a vocabulary. The vocabulary object can:\n",
    "* Tell you all the words: `get_words()`\n",
    "* Tell you how many words there are: `num_words()`\n",
    "* Map a word to an index: `word2index()`\n",
    "* Map an index to a word: `index2word()`\n",
    "\n",
    "Additionally, it has two helper functions used during set up:\n",
    "* `add_word()` adds a word to the vocabulary.\n",
    "* `add_sentence()` adds all the previously unknown words in a sentence to the vocabulary (simply splitting the sentence by blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "id": "e7CBDQ5F21QV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL BUT DO NOT EDIT IT\n",
    "UNK_token = 0   # Unknown '<unk>'\n",
    "UNK_symbol = '<unk>'\n",
    "\n",
    "class Vocab:\n",
    "  def __init__(self, name=''):\n",
    "    self.name = name\n",
    "    self._word2index = {UNK_symbol: UNK_token}\n",
    "    self._word2count = {UNK_symbol: 0}\n",
    "    self._index2word = {UNK_token: UNK_symbol}\n",
    "    self._n_words = 1\n",
    "\n",
    "  def get_words(self):\n",
    "    return list(self._word2count.keys())\n",
    "\n",
    "  def num_words(self):\n",
    "    return self._n_words\n",
    "\n",
    "  def word2index(self, word):\n",
    "    if word in self._word2index:\n",
    "      return self._word2index[word]\n",
    "    else:\n",
    "      return self._word2index[UNK_symbol]\n",
    "\n",
    "  def index2word(self, word):\n",
    "    return self._index2word[word]\n",
    "\n",
    "  def word2count(self, word):\n",
    "    return self._word2count[word]\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self._word2index:\n",
    "      self._word2index[word] = self._n_words\n",
    "      self._word2count[word] = 1\n",
    "      self._index2word[self._n_words] = word\n",
    "      self._n_words += 1\n",
    "    else:\n",
    "      self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "JnFgIfwk_bwv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0fn9kKpcFmnI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The continuous bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kXMUVEn-zbw_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GNd7tZotL_EE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change them\n",
    "CBOW_EMBED_DIMENSIONS = 100\n",
    "CBOW_WINDOW = 4\n",
    "CBOW_MAX_LENGTH = 50\n",
    "CBOW_BATCH_SIZE = 1024\n",
    "CBOW_NUM_EPOCHS = 2\n",
    "CBOW_LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "r-o1tDEfFp5d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the CBOW model, we must prepare the data for training. The CBOW model learns to predict a word based on the words to the left and the words to the right.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `y` is the index of a word in the corpus.\n",
    "* `x` is a list of indexes of words to the left of `y` and to the right of `y`.\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=[the, quick, fox, jumped]`, `y=brown`\n",
    "2. `x=[quick, brown, jumped, over]`, `y=fox`\n",
    "3. `x=[brown, fox, over, the]`, `y=jumped`\n",
    "4. `x=[fox, jumped, the, lazy]`, `y=over`\n",
    "5. `x=[jumped, over, lazy, dog]`, `y=the`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabulary)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_cbow_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_cbow_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TOZwYPnLzf4C",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the `prep_cbow_data()` function. It takes a data frame and a tokenizer (`my_tokenizer()`) a window to either side of each word, and a max document length. The function should return two values as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zIERa_oHDjiq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_cbow_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "    data_out = []\n",
    "    vocab = Vocab()\n",
    "    ### BEGIN SOLUTION\n",
    "    for sentence in data_frame['text']:\n",
    "        # 使用分词器函数对句子进行分词\n",
    "        tokens = tokenizer_fn(sentence)\n",
    "\n",
    "        # 更新词汇表\n",
    "        for token in tokens:\n",
    "            vocab.add_word(token)\n",
    "        \n",
    "        if len(tokens)< ((window * 2) + 1):\n",
    "            continue\n",
    "            \n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]  \n",
    "            \n",
    "        # 遍历每个可能的窗口中心词\n",
    "        for i in range(window, len(tokens) - window):\n",
    "            # 收集上下文词的索引\n",
    "            context = [vocab.word2index(tokens[j]) for j in range(i-window, i+window+1) if j != i]\n",
    "\n",
    "            # 获取中心词的索引\n",
    "            target = vocab.word2index(tokens[i])\n",
    "\n",
    "            # 添加到输出数据中\n",
    "            data_out.append((context, target))\n",
    "    ### END SOLUTION\n",
    "    return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": false,
    "id": "2W6tb7s7GMjG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataframe= 36718 len data= 625869\n"
     ]
    }
   ],
   "source": [
    "CBOW_DATA, CBOW_VOCAB = prep_cbow_data(WIKI_TRAIN, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n",
    "print(\"len dataframe=\", len(WIKI_TRAIN), \"len data=\", len(CBOW_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "51d1xzfxQpFb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " <!-- Unit test: Do something along the lines of figuring out how many words are in lines with greater than window*2+1 words. What I have below isn't quite matching what my solution above is producing. I'm not sure if my solution above has a bug or if my computation below is incorrect, or if it is just an approximation and we should allow some variance. -->\n",
    " Test: checking the size of the dataset and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": false,
    "id": "mBHltjJgRgqD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected data points 625869\n",
      "actual data points 625869\n",
      "difference 0\n",
      "\n",
      "least vocab size 28782\n",
      "actual vocab size 28782\n",
      "\n",
      "Test passed!\n",
      "Test D: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test D (10 points)\n",
    "ag.check_data_size_d(WIKI_TRAIN, CBOW_WINDOW, CBOW_DATA, CBOW_VOCAB, max_length=CBOW_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GcabfK1P0EU3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Get Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqnjtfprK2jb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the following function. `get_batch()` will return a batch of data of the given size, starting at the given index.\n",
    "\n",
    "The function should return two values:\n",
    "1. A batch of `x` components of the data as a tensor of size `window*2 x batch_size`.\n",
    "2. A batch of `y` components of the data as a tensor array of length `window*2`.\n",
    "\n",
    "Both tensors should be moved to the GPU, if available, before being returned (Note: Gradescope will not have a GPU available).\n",
    "\n",
    "**Hint:** You should not need to write a loop. You can achieve what you need using numpy slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OhL7ncHyBX9y",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def get_batch(data, index, batch_size=10):\n",
    "  ### BEGIN SOLUTION\n",
    "    batch_data = data[index:index+batch_size]\n",
    "\n",
    "    # 分别获取context和target\n",
    "    context = [item[0] for item in batch_data]\n",
    "    target = [item[1] for item in batch_data]\n",
    "\n",
    "    # 转换成numpy数组以便于进一步处理\n",
    "    x = np.array(context)\n",
    "    y = np.array(target)\n",
    "\n",
    "    # 转换成PyTorch张量\n",
    "    x_tensor = torch.tensor(x, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # 检查是否有GPU可用，并相应地移动张量\n",
    "    if torch.cuda.is_available():\n",
    "        x_tensor = x_tensor.cuda()\n",
    "        y_tensor = y_tensor.cuda()\n",
    "    ### END SOLUTION\n",
    "    return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uwiTTNSlOpGK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test: make up some synthetic data, check if you get the right stuff out for a given idx and batch size. -->\n",
    "Test: Check if get back works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": false,
    "id": "CrTiQ-WOULcZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test E: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test E (10 points)\n",
    "ag.unit_test_get_batch(CBOW_DATA, CBOW_WINDOW, 10, get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "_jRn11uf0IrR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aq09TkU6UtMn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the CBOW model specification.\n",
    "\n",
    "The CBOW model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a list of `window*2` indices and produce a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "k2eQkT122Am3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = None\n",
    "        ### BEGIN SOLUTION\n",
    "        embeds = self.embeddings(x)\n",
    "        # 因为输入是上下文词的索引，所以我们需要取平均来获得中心词的向量表示\n",
    "        embeds_mean = torch.mean(embeds, dim=1)\n",
    "        # 通过线性层\n",
    "        out = self.linear(embeds_mean)\n",
    "        # 计算log softmax\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        ### END SOLUTION\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UZLSvKvWVmwT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": false,
    "id": "6vMBdIU8fz0h",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n",
    "cbow_model.to(DEVICE)\n",
    "CBOW_CRITERION = nn.NLLLoss()\n",
    "try:\n",
    "  CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "amzCDmGi6XeD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Check the structure of CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": false,
    "id": "_RVJeGf1oJla",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has two layers as expected!\n",
      "Your layers orders are as expected!\n",
      "Test F: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test F (10 points)\n",
    "ag.test_cbow_structure(cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "WcH2bQU50hJM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kvk4DBN8Vpk5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": false,
    "id": "w5kYEX41LKKj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_cbow(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item())\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ynoHvwSTVsv6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": false,
    "id": "CTpPWtKtB07T",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 10.297601699829102\n",
      "iter 100 loss 9.387812926037476\n",
      "iter 200 loss 8.172580972832828\n",
      "iter 300 loss 7.0439315064008845\n",
      "iter 400 loss 6.228400967662174\n",
      "iter 500 loss 5.6172416176862585\n",
      "iter 600 loss 5.139382269537191\n",
      "epoch 0 loss 5.097317651914106\n",
      "iter 0 loss 3.2127554416656494\n",
      "iter 100 loss 2.5136845808218022\n",
      "iter 200 loss 2.111066895930921\n",
      "iter 300 loss 1.8445323563097322\n",
      "iter 400 loss 1.6536653386684428\n",
      "iter 500 loss 1.51683737513072\n",
      "iter 600 loss 1.429860233863856\n",
      "epoch 1 loss 1.4240448332457223\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "FYtxtDr1Yf7U",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Now that we have trained the CBOW model, we will be testing it on the `WIKI_TEST` dataset. Your CBOW model will need to achieve an accuracy of at least 30% to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": false,
    "id": "Y0nbJltGqR8B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "      token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "      for i in range(window, len(token_ids)-window):\n",
    "        x = token_ids[i-window:i] + token_ids[i+1:i+window+1]\n",
    "        y = token_ids[i]\n",
    "        data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": false,
    "id": "tnVcs4HQtXV_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed! Accuracy = 0.2303483486175537/20\n",
      "Test G: 0/20\n"
     ]
    }
   ],
   "source": [
    "# student check - G (20 points)\n",
    "ag.test_cbow_performance(cbow_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 10.287550926208496\n",
      "iter 100 loss 8.17327341702905\n",
      "iter 200 loss 6.089326350843136\n",
      "iter 300 loss 4.980780759919125\n",
      "iter 400 loss 4.269579738750124\n",
      "iter 500 loss 3.7637967908691743\n",
      "iter 600 loss 3.386500017813557\n",
      "iter 700 loss 3.087695546054976\n",
      "iter 800 loss 2.8478051124887074\n",
      "iter 900 loss 2.656657671690252\n",
      "iter 1000 loss 2.4898448980057037\n",
      "iter 1100 loss 2.34948182993862\n",
      "iter 1200 loss 2.2310371388999943\n",
      "epoch 0 loss 2.207649864122247\n",
      "iter 0 loss 3.7422635555267334\n",
      "iter 100 loss 1.3314449822548593\n",
      "iter 200 loss 0.808415286057624\n",
      "iter 300 loss 0.584680957305075\n",
      "iter 400 loss 0.46039214973660775\n",
      "iter 500 loss 0.3821716453590079\n",
      "iter 600 loss 0.3271829803689645\n",
      "iter 700 loss 0.2872138677507681\n",
      "iter 800 loss 0.25666075241234565\n",
      "iter 900 loss 0.23314869682512326\n",
      "iter 1000 loss 0.21492817543528892\n",
      "iter 1100 loss 0.20111630929737173\n",
      "iter 1200 loss 0.19364050766771085\n",
      "epoch 1 loss 0.194566756136313\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15268179774284363/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.293304443359375\n",
      "iter 100 loss 8.165947116247498\n",
      "iter 200 loss 6.083628833590455\n",
      "iter 300 loss 4.975429333721681\n",
      "iter 400 loss 4.266734376215281\n",
      "iter 500 loss 3.763517799967539\n",
      "iter 600 loss 3.3864081356172355\n",
      "iter 700 loss 3.086953987918805\n",
      "iter 800 loss 2.8455657403864962\n",
      "iter 900 loss 2.651538527633718\n",
      "iter 1000 loss 2.4846702827797547\n",
      "iter 1100 loss 2.3463981023817904\n",
      "iter 1200 loss 2.2290182302734634\n",
      "epoch 0 loss 2.2059696270869327\n",
      "iter 0 loss 3.7712886333465576\n",
      "iter 100 loss 1.360796415274686\n",
      "iter 200 loss 0.8272799786495332\n",
      "iter 300 loss 0.5988503809139578\n",
      "iter 400 loss 0.47220463798081785\n",
      "iter 500 loss 0.39185418062164873\n",
      "iter 600 loss 0.33543691168013906\n",
      "iter 700 loss 0.2944249591754268\n",
      "iter 800 loss 0.2629766593990701\n",
      "iter 900 loss 0.23870895056676256\n",
      "iter 1000 loss 0.2202810358952035\n",
      "iter 1100 loss 0.20623870564118826\n",
      "iter 1200 loss 0.19891797059481586\n",
      "epoch 1 loss 0.20001973805616804\n",
      "iter 0 loss 0.9032067060470581\n",
      "iter 100 loss 0.22684065215658433\n",
      "iter 200 loss 0.13988221408938295\n",
      "iter 300 loss 0.10278067813313681\n",
      "iter 400 loss 0.08213148328784546\n",
      "iter 500 loss 0.06894274356360446\n",
      "iter 600 loss 0.059669170823078384\n",
      "iter 700 loss 0.05287581656563937\n",
      "iter 800 loss 0.047705359974371433\n",
      "iter 900 loss 0.04373430378826027\n",
      "iter 1000 loss 0.040835710194010236\n",
      "iter 1100 loss 0.03872801232863079\n",
      "iter 1200 loss 0.03795576080071936\n",
      "epoch 2 loss 0.03878052686927947\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.14490684866905212/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.274698257446289\n",
      "iter 100 loss 8.26055651372022\n",
      "iter 200 loss 6.111878109215504\n",
      "iter 300 loss 4.968043382777724\n",
      "iter 400 loss 4.252306627811042\n",
      "iter 500 loss 3.749605367283621\n",
      "iter 600 loss 3.375596003207113\n",
      "iter 700 loss 3.0779411440739106\n",
      "iter 800 loss 2.836328283082531\n",
      "iter 900 loss 2.643283794114116\n",
      "iter 1000 loss 2.4786529433834446\n",
      "iter 1100 loss 2.342561397359763\n",
      "iter 1200 loss 2.2270274381157162\n",
      "epoch 0 loss 2.2037907279357194\n",
      "iter 0 loss 3.7289397716522217\n",
      "iter 100 loss 1.3170688376568331\n",
      "iter 200 loss 0.7939236026201675\n",
      "iter 300 loss 0.5747411710231803\n",
      "iter 400 loss 0.4528019317516365\n",
      "iter 500 loss 0.37561035770439577\n",
      "iter 600 loss 0.32168073741449493\n",
      "iter 700 loss 0.2823978531441403\n",
      "iter 800 loss 0.2524641614486365\n",
      "iter 900 loss 0.22931075351236266\n",
      "iter 1000 loss 0.21176397393678095\n",
      "iter 1100 loss 0.19814578911034114\n",
      "iter 1200 loss 0.1911223390225715\n",
      "epoch 1 loss 0.19198823186421063\n",
      "iter 0 loss 0.8879405856132507\n",
      "iter 100 loss 0.22137490940271037\n",
      "iter 200 loss 0.13571617103631223\n",
      "iter 300 loss 0.09979518559882015\n",
      "iter 400 loss 0.0797882554797163\n",
      "iter 500 loss 0.06699487149061081\n",
      "iter 600 loss 0.058001970405846884\n",
      "iter 700 loss 0.05141390628910779\n",
      "iter 800 loss 0.04640445008422812\n",
      "iter 900 loss 0.04255457528630121\n",
      "iter 1000 loss 0.03974451100570964\n",
      "iter 1100 loss 0.03767082598449478\n",
      "iter 1200 loss 0.03674306487858444\n",
      "epoch 2 loss 0.037568595461583955\n",
      "iter 0 loss 0.32086747884750366\n",
      "iter 100 loss 0.06584111700701241\n",
      "iter 200 loss 0.04201105031745499\n",
      "iter 300 loss 0.03144165728414475\n",
      "iter 400 loss 0.025435730662810015\n",
      "iter 500 loss 0.021560165729046524\n",
      "iter 600 loss 0.018816465394146827\n",
      "iter 700 loss 0.01679163144106914\n",
      "iter 800 loss 0.015255649408997594\n",
      "iter 900 loss 0.01407985541334874\n",
      "iter 1000 loss 0.013248750805058381\n",
      "iter 1100 loss 0.012665241773576086\n",
      "iter 1200 loss 0.012453969124841079\n",
      "epoch 3 loss 0.012748404022278767\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.12548828125/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.319389343261719\n",
      "iter 100 loss 9.406601008802358\n",
      "iter 200 loss 8.202125131787353\n",
      "iter 300 loss 7.070755863506532\n",
      "iter 400 loss 6.277193680071177\n",
      "iter 500 loss 5.721833348512174\n",
      "iter 600 loss 5.3106194054227505\n",
      "iter 700 loss 4.980977841348689\n",
      "iter 800 loss 4.706542990776186\n",
      "iter 900 loss 4.477037749994343\n",
      "iter 1000 loss 4.278332902239515\n",
      "iter 1100 loss 4.107300196854664\n",
      "iter 1200 loss 3.9601216699360413\n",
      "epoch 0 loss 3.9303571649151814\n",
      "iter 0 loss 4.5063796043396\n",
      "iter 100 loss 2.9076115341469793\n",
      "iter 200 loss 2.1734502303659617\n",
      "iter 300 loss 1.7697756359743517\n",
      "iter 400 loss 1.5021894538491742\n",
      "iter 500 loss 1.308832657610823\n",
      "iter 600 loss 1.1612886814230095\n",
      "iter 700 loss 1.0449839389953395\n",
      "iter 800 loss 0.9512980510381872\n",
      "iter 900 loss 0.8793239207167207\n",
      "iter 1000 loss 0.8241870678685881\n",
      "iter 1100 loss 0.7857591709583917\n",
      "iter 1200 loss 0.7694807221351515\n",
      "epoch 1 loss 0.7700101803445581\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.19426082074642181/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.294386863708496\n",
      "iter 100 loss 9.402605623301893\n",
      "iter 200 loss 8.211647901962053\n",
      "iter 300 loss 7.074545042855399\n",
      "iter 400 loss 6.278154781631698\n",
      "iter 500 loss 5.721731963509809\n",
      "iter 600 loss 5.311541357373636\n",
      "iter 700 loss 4.980013419149945\n",
      "iter 800 loss 4.706034959776423\n",
      "iter 900 loss 4.47718706856028\n",
      "iter 1000 loss 4.277021292325381\n",
      "iter 1100 loss 4.104422856200077\n",
      "iter 1200 loss 3.9581370454942255\n",
      "epoch 0 loss 3.9291181556526844\n",
      "iter 0 loss 4.509825229644775\n",
      "iter 100 loss 2.8807374132741796\n",
      "iter 200 loss 2.16379038196298\n",
      "iter 300 loss 1.7616253619970277\n",
      "iter 400 loss 1.4910262264218415\n",
      "iter 500 loss 1.3004766382619055\n",
      "iter 600 loss 1.1549661412413623\n",
      "iter 700 loss 1.0408460549723235\n",
      "iter 800 loss 0.9488624285967013\n",
      "iter 900 loss 0.876856798998656\n",
      "iter 1000 loss 0.8214105247379421\n",
      "iter 1100 loss 0.7830739410815728\n",
      "iter 1200 loss 0.7690733232218261\n",
      "epoch 1 loss 0.7704300142690514\n",
      "iter 0 loss 2.2187273502349854\n",
      "iter 100 loss 1.0285407576230492\n",
      "iter 200 loss 0.6862777053420224\n",
      "iter 300 loss 0.5207143188810032\n",
      "iter 400 loss 0.4221236852933641\n",
      "iter 500 loss 0.35784659884171094\n",
      "iter 600 loss 0.3116518402729376\n",
      "iter 700 loss 0.2770974995746422\n",
      "iter 800 loss 0.25029683532135166\n",
      "iter 900 loss 0.22951648729573212\n",
      "iter 1000 loss 0.21402695551737919\n",
      "iter 1100 loss 0.2034370757294286\n",
      "iter 1200 loss 0.20237089796526844\n",
      "epoch 2 loss 0.2054740086856333\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.17308944463729858/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.279133796691895\n",
      "iter 100 loss 9.330021187810615\n",
      "iter 200 loss 8.112880830147967\n",
      "iter 300 loss 7.0077494069983395\n",
      "iter 400 loss 6.237623711179319\n",
      "iter 500 loss 5.693671561524778\n",
      "iter 600 loss 5.288921731879033\n",
      "iter 700 loss 4.967443163486759\n",
      "iter 800 loss 4.698933124839887\n",
      "iter 900 loss 4.469763440112031\n",
      "iter 1000 loss 4.272246570377559\n",
      "iter 1100 loss 4.103287028789953\n",
      "iter 1200 loss 3.958647828018735\n",
      "epoch 0 loss 3.9299263183496977\n",
      "iter 0 loss 4.490321159362793\n",
      "iter 100 loss 2.8510396244502303\n",
      "iter 200 loss 2.1286554917767275\n",
      "iter 300 loss 1.725836078589937\n",
      "iter 400 loss 1.458513168474088\n",
      "iter 500 loss 1.268308336982232\n",
      "iter 600 loss 1.1236438637168553\n",
      "iter 700 loss 1.0116124820607195\n",
      "iter 800 loss 0.9205279513616835\n",
      "iter 900 loss 0.8497373227538597\n",
      "iter 1000 loss 0.7965854936308199\n",
      "iter 1100 loss 0.7603610885294864\n",
      "iter 1200 loss 0.7478509219758814\n",
      "epoch 1 loss 0.7496461318577174\n",
      "iter 0 loss 2.149468183517456\n",
      "iter 100 loss 0.9655070027502457\n",
      "iter 200 loss 0.6386901871926749\n",
      "iter 300 loss 0.48376744668745125\n",
      "iter 400 loss 0.3925034088312539\n",
      "iter 500 loss 0.33288326918959854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600 loss 0.2900973763967711\n",
      "iter 700 loss 0.25828888513859943\n",
      "iter 800 loss 0.2334816659871633\n",
      "iter 900 loss 0.21437125037383953\n",
      "iter 1000 loss 0.20027294601653364\n",
      "iter 1100 loss 0.19072536787155214\n",
      "iter 1200 loss 0.19077561915441912\n",
      "epoch 2 loss 0.19440881526875028\n",
      "iter 0 loss 0.9271354675292969\n",
      "iter 100 loss 0.3197068833183534\n",
      "iter 200 loss 0.20632587147144535\n",
      "iter 300 loss 0.155244960613623\n",
      "iter 400 loss 0.12589289284220656\n",
      "iter 500 loss 0.10689027814153663\n",
      "iter 600 loss 0.09336610123229999\n",
      "iter 700 loss 0.083311215298152\n",
      "iter 800 loss 0.07554854295180458\n",
      "iter 900 loss 0.06959010620956019\n",
      "iter 1000 loss 0.06527675319645372\n",
      "iter 1100 loss 0.06239908765307835\n",
      "iter 1200 loss 0.06313088217239941\n",
      "epoch 3 loss 0.06544664639784496\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1430288404226303/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.290266036987305\n",
      "iter 100 loss 8.266765830540422\n",
      "iter 200 loss 6.206605042984236\n",
      "iter 300 loss 5.032163901978553\n",
      "iter 400 loss 4.257971362282808\n",
      "iter 500 loss 3.697502460784303\n",
      "iter 600 loss 3.269259241773761\n",
      "epoch 0 loss 3.23231654081329\n",
      "iter 0 loss 2.0035948753356934\n",
      "iter 100 loss 1.1064738123723776\n",
      "iter 200 loss 0.7745496429021086\n",
      "iter 300 loss 0.5969600998285997\n",
      "iter 400 loss 0.4882159876332913\n",
      "iter 500 loss 0.4207675976013233\n",
      "iter 600 loss 0.38263518662004425\n",
      "epoch 1 loss 0.38127575390370116\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1555614024400711/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.283336639404297\n",
      "iter 100 loss 8.233393914628737\n",
      "iter 200 loss 6.189924948251069\n",
      "iter 300 loss 5.034256465411265\n",
      "iter 400 loss 4.268989070395282\n",
      "iter 500 loss 3.7145852221700246\n",
      "iter 600 loss 3.2917732561685877\n",
      "epoch 0 loss 3.25509208567599\n",
      "iter 0 loss 2.0627591609954834\n",
      "iter 100 loss 1.150243600406269\n",
      "iter 200 loss 0.8131452349584494\n",
      "iter 300 loss 0.6306372647764675\n",
      "iter 400 loss 0.5192193526430915\n",
      "iter 500 loss 0.44742797802903217\n",
      "iter 600 loss 0.4064494129921156\n",
      "epoch 1 loss 0.4048944385859463\n",
      "iter 0 loss 0.6863823533058167\n",
      "iter 100 loss 0.2684611597716218\n",
      "iter 200 loss 0.17818520796387943\n",
      "iter 300 loss 0.13675258460878534\n",
      "iter 400 loss 0.11304043186944619\n",
      "iter 500 loss 0.09876739595107689\n",
      "iter 600 loss 0.09219831655951784\n",
      "epoch 2 loss 0.0931494361713358\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15330779552459717/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.30798053741455\n",
      "iter 100 loss 8.189213710256142\n",
      "iter 200 loss 6.143707749855459\n",
      "iter 300 loss 4.990459816004351\n",
      "iter 400 loss 4.227378126689026\n",
      "iter 500 loss 3.6711359019288996\n",
      "iter 600 loss 3.245316862861646\n",
      "epoch 0 loss 3.2083038537474975\n",
      "iter 0 loss 2.002274751663208\n",
      "iter 100 loss 1.08982521118504\n",
      "iter 200 loss 0.7588783361721988\n",
      "iter 300 loss 0.5849472322436267\n",
      "iter 400 loss 0.4792526487326087\n",
      "iter 500 loss 0.4127646096928153\n",
      "iter 600 loss 0.37423010304347054\n",
      "epoch 1 loss 0.3727831253492305\n",
      "iter 0 loss 0.6645511984825134\n",
      "iter 100 loss 0.25720624817479953\n",
      "iter 200 loss 0.17026594876131015\n",
      "iter 300 loss 0.13044936251996758\n",
      "iter 400 loss 0.10761704803107683\n",
      "iter 500 loss 0.09401128870879343\n",
      "iter 600 loss 0.08752263161336721\n",
      "epoch 2 loss 0.08831736025950718\n",
      "iter 0 loss 0.26158249378204346\n",
      "iter 100 loss 0.08592975434690418\n",
      "iter 200 loss 0.059083009991257346\n",
      "iter 300 loss 0.046335071201538326\n",
      "iter 400 loss 0.0389827836230583\n",
      "iter 500 loss 0.034701960083253366\n",
      "iter 600 loss 0.03287155133131241\n",
      "epoch 3 loss 0.033609156452619304\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.14067508280277252/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.290742874145508\n",
      "iter 100 loss 9.403752270311411\n",
      "iter 200 loss 8.2065604100773\n",
      "iter 300 loss 7.06572022390524\n",
      "iter 400 loss 6.238590714937434\n",
      "iter 500 loss 5.622717387186077\n",
      "iter 600 loss 5.140391163738714\n",
      "epoch 0 loss 5.097762830128646\n",
      "iter 0 loss 3.2047314643859863\n",
      "iter 100 loss 2.510975975801449\n",
      "iter 200 loss 2.108180302292553\n",
      "iter 300 loss 1.8454886849140408\n",
      "iter 400 loss 1.6576831429973802\n",
      "iter 500 loss 1.520879469945759\n",
      "iter 600 loss 1.4312430613251177\n",
      "epoch 1 loss 1.4249423989102414\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.18731220066547394/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.303683280944824\n",
      "iter 100 loss 9.335763515812335\n",
      "iter 200 loss 8.090093218865086\n",
      "iter 300 loss 6.984014682199472\n",
      "iter 400 loss 6.186594892321085\n",
      "iter 500 loss 5.583882397520328\n",
      "iter 600 loss 5.109399652322398\n",
      "epoch 0 loss 5.067370988343234\n",
      "iter 0 loss 3.247849464416504\n",
      "iter 100 loss 2.534370207550502\n",
      "iter 200 loss 2.128551066811405\n",
      "iter 300 loss 1.864314490378497\n",
      "iter 400 loss 1.6747484638209356\n",
      "iter 500 loss 1.5361272972263025\n",
      "iter 600 loss 1.4446659260700625\n",
      "epoch 1 loss 1.438108382950828\n",
      "iter 0 loss 1.6536403894424438\n",
      "iter 100 loss 1.089847462012036\n",
      "iter 200 loss 0.8412121669273471\n",
      "iter 300 loss 0.6949490779064026\n",
      "iter 400 loss 0.5993044701151717\n",
      "iter 500 loss 0.5366433673871016\n",
      "iter 600 loss 0.5054457597248566\n",
      "epoch 2 loss 0.5048355966952741\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.17570613324642181/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.275605201721191\n",
      "iter 100 loss 9.351086389900434\n",
      "iter 200 loss 8.111530102307524\n",
      "iter 300 loss 6.984433568593276\n",
      "iter 400 loss 6.180828040377458\n",
      "iter 500 loss 5.580711161543033\n",
      "iter 600 loss 5.110863684021098\n",
      "epoch 0 loss 5.069383211885068\n",
      "iter 0 loss 3.213566303253174\n",
      "iter 100 loss 2.5239317511567974\n",
      "iter 200 loss 2.120776748775843\n",
      "iter 300 loss 1.859223947968594\n",
      "iter 400 loss 1.6720782191378816\n",
      "iter 500 loss 1.538030410241224\n",
      "iter 600 loss 1.4516697206632072\n",
      "epoch 1 loss 1.445719613673839\n",
      "iter 0 loss 1.6458865404129028\n",
      "iter 100 loss 1.0992366453208546\n",
      "iter 200 loss 0.8509689096491135\n",
      "iter 300 loss 0.704779175230831\n",
      "iter 400 loss 0.6104961822455066\n",
      "iter 500 loss 0.5510817243905363\n",
      "iter 600 loss 0.5226344226699899\n",
      "epoch 2 loss 0.5223920692593689\n",
      "iter 0 loss 0.8313137888908386\n",
      "iter 100 loss 0.45685224397347707\n",
      "iter 200 loss 0.3297911264854877\n",
      "iter 300 loss 0.2644472477889536\n",
      "iter 400 loss 0.22545724126168915\n",
      "iter 500 loss 0.20324782326728283\n",
      "iter 600 loss 0.19703770826838773\n",
      "epoch 3 loss 0.19852538855590915\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1588166058063507/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.291216850280762\n",
      "iter 100 loss 8.108643017192879\n",
      "iter 200 loss 6.016501362644025\n",
      "iter 300 loss 4.906384461741907\n",
      "iter 400 loss 4.2053931817746815\n",
      "iter 500 loss 3.7041784384531415\n",
      "iter 600 loss 3.332761080610177\n",
      "iter 700 loss 3.03892351591298\n",
      "iter 800 loss 2.802114185917839\n",
      "iter 900 loss 2.611266335160301\n",
      "iter 1000 loss 2.449485568018941\n",
      "iter 1100 loss 2.3173710097517346\n",
      "iter 1200 loss 2.204626980669592\n",
      "epoch 0 loss 2.1816755834655948\n",
      "iter 0 loss 3.8640098571777344\n",
      "iter 100 loss 1.350054575960235\n",
      "iter 200 loss 0.8170756142382598\n",
      "iter 300 loss 0.5907844231770284\n",
      "iter 400 loss 0.46478949974005357\n",
      "iter 500 loss 0.3847020064850529\n",
      "iter 600 loss 0.32910286780824877\n",
      "iter 700 loss 0.2887302053914261\n",
      "iter 800 loss 0.2578736206100377\n",
      "iter 900 loss 0.23425950614795435\n",
      "iter 1000 loss 0.2162481633486686\n",
      "iter 1100 loss 0.20257058223670554\n",
      "iter 1200 loss 0.19540518181372146\n",
      "epoch 1 loss 0.1960000091928946\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15833890438079834/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.27703857421875\n",
      "iter 100 loss 8.172864460709071\n",
      "iter 200 loss 6.100850726834577\n",
      "iter 300 loss 4.977883391998139\n",
      "iter 400 loss 4.262800329046653\n",
      "iter 500 loss 3.756137175712281\n",
      "iter 600 loss 3.3802200968372644\n",
      "iter 700 loss 3.0800725597457776\n",
      "iter 800 loss 2.836878807654839\n",
      "iter 900 loss 2.640675025563129\n",
      "iter 1000 loss 2.4742404620249667\n",
      "iter 1100 loss 2.3338829419701668\n",
      "iter 1200 loss 2.214580416629753\n",
      "epoch 0 loss 2.1910778101649493\n",
      "iter 0 loss 3.775177240371704\n",
      "iter 100 loss 1.3317414259556497\n",
      "iter 200 loss 0.8051723839335181\n",
      "iter 300 loss 0.5824174937665264\n",
      "iter 400 loss 0.45889805554600427\n",
      "iter 500 loss 0.38017756105986183\n",
      "iter 600 loss 0.3254495363205225\n",
      "iter 700 loss 0.2854775567346395\n",
      "iter 800 loss 0.25497037366255837\n",
      "iter 900 loss 0.2313948130435605\n",
      "iter 1000 loss 0.21331050840544177\n",
      "iter 1100 loss 0.19949872882446304\n",
      "iter 1200 loss 0.19237822502677784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.19334566382212648\n",
      "iter 0 loss 0.8933827877044678\n",
      "iter 100 loss 0.2213100191242624\n",
      "iter 200 loss 0.1362816919549484\n",
      "iter 300 loss 0.1002413431287703\n",
      "iter 400 loss 0.08018066070769493\n",
      "iter 500 loss 0.06725810535861823\n",
      "iter 600 loss 0.05819078899002611\n",
      "iter 700 loss 0.05153594035841259\n",
      "iter 800 loss 0.046495007508005316\n",
      "iter 900 loss 0.04261400237406597\n",
      "iter 1000 loss 0.03973155448884963\n",
      "iter 1100 loss 0.03764324907812338\n",
      "iter 1200 loss 0.03678280195309707\n",
      "epoch 2 loss 0.037554570854795445\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15286597609519958/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.296178817749023\n",
      "iter 100 loss 8.109961665502869\n",
      "iter 200 loss 6.042489512049737\n",
      "iter 300 loss 4.954100458328906\n",
      "iter 400 loss 4.26345232508129\n",
      "iter 500 loss 3.7669895354383245\n",
      "iter 600 loss 3.3938020114692398\n",
      "iter 700 loss 3.093021508630434\n",
      "iter 800 loss 2.849559129847123\n",
      "iter 900 loss 2.6539963336948813\n",
      "iter 1000 loss 2.4875096205945733\n",
      "iter 1100 loss 2.349303095910248\n",
      "iter 1200 loss 2.2328071774888496\n",
      "epoch 0 loss 2.2097516014513525\n",
      "iter 0 loss 3.6962461471557617\n",
      "iter 100 loss 1.3383466506948565\n",
      "iter 200 loss 0.8118510270741448\n",
      "iter 300 loss 0.5870487675120268\n",
      "iter 400 loss 0.4623733663053584\n",
      "iter 500 loss 0.38330540472191726\n",
      "iter 600 loss 0.3279911961511844\n",
      "iter 700 loss 0.2877048308962422\n",
      "iter 800 loss 0.2569234689858076\n",
      "iter 900 loss 0.2331711656716833\n",
      "iter 1000 loss 0.21494442207755504\n",
      "iter 1100 loss 0.20114465868548845\n",
      "iter 1200 loss 0.1940721742914281\n",
      "epoch 1 loss 0.1952316986851138\n",
      "iter 0 loss 0.8472579121589661\n",
      "iter 100 loss 0.21405307461719703\n",
      "iter 200 loss 0.13242743425627254\n",
      "iter 300 loss 0.09752722823392117\n",
      "iter 400 loss 0.07797902565625986\n",
      "iter 500 loss 0.0654729203824809\n",
      "iter 600 loss 0.056677486183441025\n",
      "iter 700 loss 0.050231589376395\n",
      "iter 800 loss 0.04533015272246392\n",
      "iter 900 loss 0.0415674542055179\n",
      "iter 1000 loss 0.03879653279679817\n",
      "iter 1100 loss 0.036802561426675665\n",
      "iter 1200 loss 0.03601780870105826\n",
      "epoch 2 loss 0.036915720415297046\n",
      "iter 0 loss 0.29040777683258057\n",
      "iter 100 loss 0.06333052953428561\n",
      "iter 200 loss 0.04080172206984082\n",
      "iter 300 loss 0.03059253844113841\n",
      "iter 400 loss 0.024760905395440777\n",
      "iter 500 loss 0.02098791876960241\n",
      "iter 600 loss 0.018316082822967834\n",
      "iter 700 loss 0.0163434649500119\n",
      "iter 800 loss 0.014850109211574966\n",
      "iter 900 loss 0.013710408118014926\n",
      "iter 1000 loss 0.012895340879134217\n",
      "iter 1100 loss 0.01233957674723122\n",
      "iter 1200 loss 0.01215363352355562\n",
      "epoch 3 loss 0.012472419031635049\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.13346005976200104/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.28292179107666\n",
      "iter 100 loss 9.385399308535131\n",
      "iter 200 loss 8.18941667661145\n",
      "iter 300 loss 7.073278232270301\n",
      "iter 400 loss 6.285702893264276\n",
      "iter 500 loss 5.725416873981377\n",
      "iter 600 loss 5.309349321089251\n",
      "iter 700 loss 4.975547260632698\n",
      "iter 800 loss 4.700411897175917\n",
      "iter 900 loss 4.471619245611735\n",
      "iter 1000 loss 4.2738228244381355\n",
      "iter 1100 loss 4.102870498106331\n",
      "iter 1200 loss 3.957323063223884\n",
      "epoch 0 loss 3.9284594293116744\n",
      "iter 0 loss 4.427023887634277\n",
      "iter 100 loss 2.8328118017404385\n",
      "iter 200 loss 2.102035805360595\n",
      "iter 300 loss 1.7012205448657571\n",
      "iter 400 loss 1.4344851594613377\n",
      "iter 500 loss 1.247922042886654\n",
      "iter 600 loss 1.1055478289004372\n",
      "iter 700 loss 0.9937429309486493\n",
      "iter 800 loss 0.9049211641524764\n",
      "iter 900 loss 0.8354137735679068\n",
      "iter 1000 loss 0.7819889776416116\n",
      "iter 1100 loss 0.7468784579518273\n",
      "iter 1200 loss 0.733950645376701\n",
      "epoch 1 loss 0.7354105966814997\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1888682246208191/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.295763969421387\n",
      "iter 100 loss 9.336769717754704\n",
      "iter 200 loss 8.125632141360002\n",
      "iter 300 loss 7.015433072251735\n",
      "iter 400 loss 6.241077763778611\n",
      "iter 500 loss 5.6920335316610435\n",
      "iter 600 loss 5.280313306560929\n",
      "iter 700 loss 4.946909946993992\n",
      "iter 800 loss 4.672319925977347\n",
      "iter 900 loss 4.441980898314125\n",
      "iter 1000 loss 4.2427114254230265\n",
      "iter 1100 loss 4.074368202502245\n",
      "iter 1200 loss 3.9308414606130886\n",
      "epoch 0 loss 3.9021695436705546\n",
      "iter 0 loss 4.402640342712402\n",
      "iter 100 loss 2.8378101528281032\n",
      "iter 200 loss 2.129787630109645\n",
      "iter 300 loss 1.7357031429724836\n",
      "iter 400 loss 1.4737984732796723\n",
      "iter 500 loss 1.2828039219993317\n",
      "iter 600 loss 1.137666751263344\n",
      "iter 700 loss 1.0253110810710429\n",
      "iter 800 loss 0.9347164622704486\n",
      "iter 900 loss 0.8635173319579493\n",
      "iter 1000 loss 0.8094221337751433\n",
      "iter 1100 loss 0.7723266127757004\n",
      "iter 1200 loss 0.7567299567194009\n",
      "epoch 1 loss 0.7576457741998026\n",
      "iter 0 loss 2.1477127075195312\n",
      "iter 100 loss 0.9826464313681763\n",
      "iter 200 loss 0.6542274839249417\n",
      "iter 300 loss 0.4968473932077718\n",
      "iter 400 loss 0.4036134847084483\n",
      "iter 500 loss 0.3422178453552033\n",
      "iter 600 loss 0.2981984326972343\n",
      "iter 700 loss 0.2653722018853744\n",
      "iter 800 loss 0.23990343267867775\n",
      "iter 900 loss 0.2200152006674223\n",
      "iter 1000 loss 0.20510362322022627\n",
      "iter 1100 loss 0.1949444430272055\n",
      "iter 1200 loss 0.1931145965438848\n",
      "epoch 2 loss 0.19594716818762392\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15882979333400726/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.307690620422363\n",
      "iter 100 loss 9.386868571290876\n",
      "iter 200 loss 8.187612486122852\n",
      "iter 300 loss 7.068728370919972\n",
      "iter 400 loss 6.27628092991741\n",
      "iter 500 loss 5.715544216171234\n",
      "iter 600 loss 5.297766578772698\n",
      "iter 700 loss 4.960747348428963\n",
      "iter 800 loss 4.687298743465867\n",
      "iter 900 loss 4.458999658662921\n",
      "iter 1000 loss 4.261459270080962\n",
      "iter 1100 loss 4.093237498800502\n",
      "iter 1200 loss 3.947916613927392\n",
      "epoch 0 loss 3.918763117001997\n",
      "iter 0 loss 4.450672149658203\n",
      "iter 100 loss 2.837605066818766\n",
      "iter 200 loss 2.111310252502783\n",
      "iter 300 loss 1.7061730090565856\n",
      "iter 400 loss 1.4392663233000738\n",
      "iter 500 loss 1.250171800394972\n",
      "iter 600 loss 1.1078019902829124\n",
      "iter 700 loss 0.9972842677513645\n",
      "iter 800 loss 0.9093201164151548\n",
      "iter 900 loss 0.840001576682439\n",
      "iter 1000 loss 0.7867942543534727\n",
      "iter 1100 loss 0.7512030807221401\n",
      "iter 1200 loss 0.7373397471406478\n",
      "epoch 1 loss 0.7385567059395941\n",
      "iter 0 loss 2.118363618850708\n",
      "iter 100 loss 0.9558983985150215\n",
      "iter 200 loss 0.6315840459729901\n",
      "iter 300 loss 0.4790865170005152\n",
      "iter 400 loss 0.3888249643277052\n",
      "iter 500 loss 0.3295776067141763\n",
      "iter 600 loss 0.2871315459071499\n",
      "iter 700 loss 0.2555553770269375\n",
      "iter 800 loss 0.23118961178296515\n",
      "iter 900 loss 0.21220337743400602\n",
      "iter 1000 loss 0.19817126374114882\n",
      "iter 1100 loss 0.18872258406839623\n",
      "iter 1200 loss 0.18767917808843196\n",
      "epoch 2 loss 0.19076544187853262\n",
      "iter 0 loss 0.875610888004303\n",
      "iter 100 loss 0.3077570868305641\n",
      "iter 200 loss 0.19938623564160285\n",
      "iter 300 loss 0.15062579434624937\n",
      "iter 400 loss 0.1223380570752811\n",
      "iter 500 loss 0.10391418030578457\n",
      "iter 600 loss 0.09076504074285015\n",
      "iter 700 loss 0.08100603733705981\n",
      "iter 800 loss 0.07351908319805296\n",
      "iter 900 loss 0.06770784371676178\n",
      "iter 1000 loss 0.06350017199618416\n",
      "iter 1100 loss 0.06076692526721879\n",
      "iter 1200 loss 0.061046959329815235\n",
      "epoch 3 loss 0.0631158107605118\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.16190049052238464/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.32097053527832\n",
      "iter 100 loss 8.237240257829722\n",
      "iter 200 loss 6.196359709127625\n",
      "iter 300 loss 5.033221930760482\n",
      "iter 400 loss 4.266416859448402\n",
      "iter 500 loss 3.710217296006437\n",
      "iter 600 loss 3.2847224329156606\n",
      "epoch 0 loss 3.2477331980940947\n",
      "iter 0 loss 2.0560381412506104\n",
      "iter 100 loss 1.1363662611139882\n",
      "iter 200 loss 0.798160962204435\n",
      "iter 300 loss 0.6167360286280958\n",
      "iter 400 loss 0.505761282216581\n",
      "iter 500 loss 0.43583489368537703\n",
      "iter 600 loss 0.3952957926296355\n",
      "epoch 1 loss 0.39385996888195046\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15502798557281494/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.288424491882324\n",
      "iter 100 loss 8.285614580211073\n",
      "iter 200 loss 6.218871849686352\n",
      "iter 300 loss 5.046522993582031\n",
      "iter 400 loss 4.275023051925431\n",
      "iter 500 loss 3.713751143324161\n",
      "iter 600 loss 3.2863301538588003\n",
      "epoch 0 loss 3.24931133222658\n",
      "iter 0 loss 2.014522075653076\n",
      "iter 100 loss 1.120482342668099\n",
      "iter 200 loss 0.7868183418292904\n",
      "iter 300 loss 0.6073792200052857\n",
      "iter 400 loss 0.49770278838507254\n",
      "iter 500 loss 0.42859871703350616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600 loss 0.3901377481450257\n",
      "epoch 1 loss 0.3888000771769135\n",
      "iter 0 loss 0.656978189945221\n",
      "iter 100 loss 0.2582020845153544\n",
      "iter 200 loss 0.1715570223049738\n",
      "iter 300 loss 0.13169425763066028\n",
      "iter 400 loss 0.1087870568111353\n",
      "iter 500 loss 0.09513348733593603\n",
      "iter 600 loss 0.08932927152926037\n",
      "epoch 2 loss 0.09026994641547118\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.14271390438079834/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.286860466003418\n",
      "iter 100 loss 8.20759608958027\n",
      "iter 200 loss 6.162895964152777\n",
      "iter 300 loss 5.006590302204373\n",
      "iter 400 loss 4.241368118664273\n",
      "iter 500 loss 3.68441941043336\n",
      "iter 600 loss 3.26113613294484\n",
      "epoch 0 loss 3.224379117867365\n",
      "iter 0 loss 2.0357210636138916\n",
      "iter 100 loss 1.1220571717413346\n",
      "iter 200 loss 0.7841871732206487\n",
      "iter 300 loss 0.6047314636632057\n",
      "iter 400 loss 0.4954892878148918\n",
      "iter 500 loss 0.4272798923794143\n",
      "iter 600 loss 0.38814002344889964\n",
      "epoch 1 loss 0.38659700804958563\n",
      "iter 0 loss 0.6795527935028076\n",
      "iter 100 loss 0.2633159108976326\n",
      "iter 200 loss 0.17380883970960456\n",
      "iter 300 loss 0.13306615516493883\n",
      "iter 400 loss 0.10969935554518664\n",
      "iter 500 loss 0.09574069956432797\n",
      "iter 600 loss 0.08948329371243666\n",
      "epoch 2 loss 0.09034943713542873\n",
      "iter 0 loss 0.2726762294769287\n",
      "iter 100 loss 0.08902755892365286\n",
      "iter 200 loss 0.06064650238449894\n",
      "iter 300 loss 0.047425025770830555\n",
      "iter 400 loss 0.03981278281649598\n",
      "iter 500 loss 0.035346433208017294\n",
      "iter 600 loss 0.03363126758393055\n",
      "epoch 3 loss 0.0343939170101124\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.15617689490318298/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.302362442016602\n",
      "iter 100 loss 9.407512098255724\n",
      "iter 200 loss 8.19808886893353\n",
      "iter 300 loss 7.0694566232421465\n",
      "iter 400 loss 6.25017424176756\n",
      "iter 500 loss 5.6361669432855175\n",
      "iter 600 loss 5.153961098729671\n",
      "epoch 0 loss 5.111482321728271\n",
      "iter 0 loss 3.2317919731140137\n",
      "iter 100 loss 2.5280444799083295\n",
      "iter 200 loss 2.115911949926348\n",
      "iter 300 loss 1.8479884617352407\n",
      "iter 400 loss 1.6561073263981694\n",
      "iter 500 loss 1.51727180530925\n",
      "iter 600 loss 1.4257523014819167\n",
      "epoch 1 loss 1.4195375546886175\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.22629094123840332/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.298576354980469\n",
      "iter 100 loss 9.351734397434953\n",
      "iter 200 loss 8.119070216790954\n",
      "iter 300 loss 7.005123461599762\n",
      "iter 400 loss 6.208523850191264\n",
      "iter 500 loss 5.607475708106796\n",
      "iter 600 loss 5.133184478207555\n",
      "epoch 0 loss 5.091398115048822\n",
      "iter 0 loss 3.2582790851593018\n",
      "iter 100 loss 2.537020497983045\n",
      "iter 200 loss 2.1302766349185167\n",
      "iter 300 loss 1.8633086752653913\n",
      "iter 400 loss 1.672581634319334\n",
      "iter 500 loss 1.5344473001009928\n",
      "iter 600 loss 1.445109595673255\n",
      "epoch 1 loss 1.4391462377557582\n",
      "iter 0 loss 1.653944730758667\n",
      "iter 100 loss 1.0829379794621232\n",
      "iter 200 loss 0.8384616223733816\n",
      "iter 300 loss 0.6935707643578615\n",
      "iter 400 loss 0.5994615825334392\n",
      "iter 500 loss 0.5391321727734601\n",
      "iter 600 loss 0.5106083845437664\n",
      "epoch 2 loss 0.5105202808707904\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.19719251990318298/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.296489715576172\n",
      "iter 100 loss 9.347640651287419\n",
      "iter 200 loss 8.099291013841013\n",
      "iter 300 loss 6.978887144513305\n",
      "iter 400 loss 6.18063085869958\n",
      "iter 500 loss 5.583204548753902\n",
      "iter 600 loss 5.111383008877569\n",
      "epoch 0 loss 5.069721001454929\n",
      "iter 0 loss 3.2175662517547607\n",
      "iter 100 loss 2.517036495822491\n",
      "iter 200 loss 2.11077686862566\n",
      "iter 300 loss 1.84773322870565\n",
      "iter 400 loss 1.6624708612660815\n",
      "iter 500 loss 1.5281651350790393\n",
      "iter 600 loss 1.4388911174656747\n",
      "epoch 1 loss 1.432864860700117\n",
      "iter 0 loss 1.6323939561843872\n",
      "iter 100 loss 1.0796979730672176\n",
      "iter 200 loss 0.8356604841514607\n",
      "iter 300 loss 0.6922894083582286\n",
      "iter 400 loss 0.5994526380464026\n",
      "iter 500 loss 0.5394364688210859\n",
      "iter 600 loss 0.5095753144107127\n",
      "epoch 2 loss 0.5094059552088892\n",
      "iter 0 loss 0.8228543400764465\n",
      "iter 100 loss 0.44822006119359836\n",
      "iter 200 loss 0.32360548099771663\n",
      "iter 300 loss 0.2597562138770902\n",
      "iter 400 loss 0.22129366829285302\n",
      "iter 500 loss 0.1986317421772523\n",
      "iter 600 loss 0.1911284557842772\n",
      "epoch 3 loss 0.1927165373208675\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.19111379981040955/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.294490814208984\n",
      "iter 100 loss 8.146081310687679\n",
      "iter 200 loss 6.041960382936013\n",
      "iter 300 loss 4.924844878060477\n",
      "iter 400 loss 4.220125048237846\n",
      "iter 500 loss 3.7211051643965485\n",
      "iter 600 loss 3.351700531067745\n",
      "iter 700 loss 3.0511998031686955\n",
      "iter 800 loss 2.810795775811175\n",
      "iter 900 loss 2.6217033892175334\n",
      "iter 1000 loss 2.4613874954777164\n",
      "iter 1100 loss 2.326756592721532\n",
      "iter 1200 loss 2.2107101841135686\n",
      "epoch 0 loss 2.187479517561324\n",
      "iter 0 loss 3.6940221786499023\n",
      "iter 100 loss 1.2955264351745643\n",
      "iter 200 loss 0.7822022332777432\n",
      "iter 300 loss 0.565274896118728\n",
      "iter 400 loss 0.44536662750187656\n",
      "iter 500 loss 0.36943576832909786\n",
      "iter 600 loss 0.3164797713437711\n",
      "iter 700 loss 0.27759995396526665\n",
      "iter 800 loss 0.2480707900847463\n",
      "iter 900 loss 0.2255962089191994\n",
      "iter 1000 loss 0.20840582148461315\n",
      "iter 1100 loss 0.19530606879214066\n",
      "iter 1200 loss 0.18859597320027097\n",
      "epoch 1 loss 0.1893123865718976\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1687825471162796/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.2763090133667\n",
      "iter 100 loss 8.175027988924839\n",
      "iter 200 loss 6.080008510333388\n",
      "iter 300 loss 4.959532386044727\n",
      "iter 400 loss 4.250897642085677\n",
      "iter 500 loss 3.747923476491384\n",
      "iter 600 loss 3.3771482698135884\n",
      "iter 700 loss 3.0805433293041933\n",
      "iter 800 loss 2.8388224341598494\n",
      "iter 900 loss 2.6490398681124097\n",
      "iter 1000 loss 2.486312198114919\n",
      "iter 1100 loss 2.3510753964966367\n",
      "iter 1200 loss 2.2335472870230375\n",
      "epoch 0 loss 2.209754558124636\n",
      "iter 0 loss 3.761380434036255\n",
      "iter 100 loss 1.3230890938551119\n",
      "iter 200 loss 0.8041814616070458\n",
      "iter 300 loss 0.5824319175955069\n",
      "iter 400 loss 0.4595541104934459\n",
      "iter 500 loss 0.3817705043002279\n",
      "iter 600 loss 0.32724995104698296\n",
      "iter 700 loss 0.2873244788949398\n",
      "iter 800 loss 0.2568713389802143\n",
      "iter 900 loss 0.23348125697554944\n",
      "iter 1000 loss 0.21573670391555314\n",
      "iter 1100 loss 0.20205188804152985\n",
      "iter 1200 loss 0.19422022078642243\n",
      "epoch 1 loss 0.19489982826315147\n",
      "iter 0 loss 0.853405237197876\n",
      "iter 100 loss 0.2112956840065446\n",
      "iter 200 loss 0.13197846543877872\n",
      "iter 300 loss 0.09733250218001117\n",
      "iter 400 loss 0.07791379172159847\n",
      "iter 500 loss 0.06551429017738787\n",
      "iter 600 loss 0.05680548916640575\n",
      "iter 700 loss 0.05039980367872965\n",
      "iter 800 loss 0.04552872476869085\n",
      "iter 900 loss 0.041806508784909026\n",
      "iter 1000 loss 0.03912695520162047\n",
      "iter 1100 loss 0.037153708761207736\n",
      "iter 1200 loss 0.03618965549650389\n",
      "epoch 2 loss 0.03680570955722061\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1376953125/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.289327621459961\n",
      "iter 100 loss 8.214775151545458\n",
      "iter 200 loss 6.1035555013969764\n",
      "iter 300 loss 4.977102863432165\n",
      "iter 400 loss 4.26305615366843\n",
      "iter 500 loss 3.7548911262177183\n",
      "iter 600 loss 3.3815794968962076\n",
      "iter 700 loss 3.086427494734738\n",
      "iter 800 loss 2.847646750909708\n",
      "iter 900 loss 2.655513808809295\n",
      "iter 1000 loss 2.48952816654514\n",
      "iter 1100 loss 2.3513172261288338\n",
      "iter 1200 loss 2.2335329702950637\n",
      "epoch 0 loss 2.209875001405928\n",
      "iter 0 loss 3.8385391235351562\n",
      "iter 100 loss 1.3506554055331956\n",
      "iter 200 loss 0.8162853655530445\n",
      "iter 300 loss 0.5914001009888031\n",
      "iter 400 loss 0.4658407057052539\n",
      "iter 500 loss 0.3863649793088198\n",
      "iter 600 loss 0.33107003987380945\n",
      "iter 700 loss 0.2907840136263238\n",
      "iter 800 loss 0.2597739814539974\n",
      "iter 900 loss 0.2357854715338624\n",
      "iter 1000 loss 0.21747696921452656\n",
      "iter 1100 loss 0.20374565787417145\n",
      "iter 1200 loss 0.19621484475419682\n",
      "epoch 1 loss 0.19720849514300226\n",
      "iter 0 loss 0.8726345896720886\n",
      "iter 100 loss 0.22041791227489416\n",
      "iter 200 loss 0.13621461287659792\n",
      "iter 300 loss 0.10032809515827122\n",
      "iter 400 loss 0.080157021320706\n",
      "iter 500 loss 0.06725186650490689\n",
      "iter 600 loss 0.05823844844807007\n",
      "iter 700 loss 0.05167543649482149\n",
      "iter 800 loss 0.04666084755263227\n",
      "iter 900 loss 0.04280224688905193\n",
      "iter 1000 loss 0.03996817161004384\n",
      "iter 1100 loss 0.037949203986414014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1200 loss 0.03702660749583419\n",
      "epoch 2 loss 0.0379113306987988\n",
      "iter 0 loss 0.2859059274196625\n",
      "iter 100 loss 0.06314108015434576\n",
      "iter 200 loss 0.04092863529679639\n",
      "iter 300 loss 0.030761428245426808\n",
      "iter 400 loss 0.024913573130138422\n",
      "iter 500 loss 0.0211209000995431\n",
      "iter 600 loss 0.018448455037608023\n",
      "iter 700 loss 0.016489295280284744\n",
      "iter 800 loss 0.01500287799794836\n",
      "iter 900 loss 0.01386676763766316\n",
      "iter 1000 loss 0.013060605810763506\n",
      "iter 1100 loss 0.012516010870698578\n",
      "iter 1200 loss 0.012317693048621296\n",
      "epoch 3 loss 0.0126405006721189\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.12552444636821747/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.322685241699219\n",
      "iter 100 loss 9.346929153593459\n",
      "iter 200 loss 8.090477070405116\n",
      "iter 300 loss 6.970191198329989\n",
      "iter 400 loss 6.200864438106889\n",
      "iter 500 loss 5.659141133645337\n",
      "iter 600 loss 5.256389058965216\n",
      "iter 700 loss 4.933627538776261\n",
      "iter 800 loss 4.667513993795445\n",
      "iter 900 loss 4.443739452319722\n",
      "iter 1000 loss 4.2488367319345235\n",
      "iter 1100 loss 4.079492231155936\n",
      "iter 1200 loss 3.9350564442109706\n",
      "epoch 0 loss 3.906199904978763\n",
      "iter 0 loss 4.502399921417236\n",
      "iter 100 loss 2.8695480965151647\n",
      "iter 200 loss 2.139577236934681\n",
      "iter 300 loss 1.7385220810820494\n",
      "iter 400 loss 1.4744272394073277\n",
      "iter 500 loss 1.2850345814537383\n",
      "iter 600 loss 1.1404503328034565\n",
      "iter 700 loss 1.0284974816522312\n",
      "iter 800 loss 0.9382132458850537\n",
      "iter 900 loss 0.867998912947027\n",
      "iter 1000 loss 0.8139915020315798\n",
      "iter 1100 loss 0.7758365417208919\n",
      "iter 1200 loss 0.7617521637385334\n",
      "epoch 1 loss 0.7629371408354826\n",
      "Config: Embed: 100, Window: 3, Batch: 512, LR: 0.001, Epochs: 2\n",
      "Test failed! Accuracy = 0.1722005158662796/20\n",
      "Test G: 0/20\n",
      "iter 0 loss 10.311588287353516\n",
      "iter 100 loss 9.30669597587963\n",
      "iter 200 loss 8.06623305372931\n",
      "iter 300 loss 6.977616905770033\n",
      "iter 400 loss 6.215955947105427\n",
      "iter 500 loss 5.671090190758011\n",
      "iter 600 loss 5.267819139207659\n",
      "iter 700 loss 4.944480202167418\n",
      "iter 800 loss 4.676918430423617\n",
      "iter 900 loss 4.449592900990646\n",
      "iter 1000 loss 4.254389370357121\n",
      "iter 1100 loss 4.088374601939285\n",
      "iter 1200 loss 3.945119956252378\n",
      "epoch 0 loss 3.9160621394502355\n",
      "iter 0 loss 4.469688892364502\n",
      "iter 100 loss 2.834711662613519\n",
      "iter 200 loss 2.1046715773160183\n",
      "iter 300 loss 1.7086667507589854\n",
      "iter 400 loss 1.4478573822915703\n",
      "iter 500 loss 1.2615102952468895\n",
      "iter 600 loss 1.1201696107768377\n",
      "iter 700 loss 1.0099891836564994\n",
      "iter 800 loss 0.9208180138308755\n"
     ]
    }
   ],
   "source": [
    "# 定义超参数的范围\n",
    "embedding_dimensions_options = [100, 200]\n",
    "window_options = [3, 4, 5]\n",
    "MAX_LENGTH_options = [50, 60, 70, 80]\n",
    "batch_size_options = [512, 1024]\n",
    "learning_rate_options = [1e-3, 5e-4]\n",
    "num_epochs_options = [2, 3, 4]\n",
    "\n",
    "# 准备记录最佳配置\n",
    "best_accuracy = 0\n",
    "best_config = {}\n",
    "\n",
    "# 超参数搜索\n",
    "for CBOW_EMBED_DIMENSIONS in embedding_dimensions_options:\n",
    "    for CBOW_WINDOW in window_options:\n",
    "        for CBOW_MAX_LENGTH in MAX_LENGTH_options:        \n",
    "            for CBOW_BATCH_SIZE in batch_size_options:\n",
    "                for CBOW_LEARNING_RATE in learning_rate_options:\n",
    "                    for CBOW_NUM_EPOCHS in num_epochs_options:\n",
    "                        cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n",
    "                        cbow_model.to(DEVICE)\n",
    "                        CBOW_CRITERION = nn.NLLLoss()\n",
    "\n",
    "                        CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "                        train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)\n",
    "\n",
    "                        TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n",
    "\n",
    "                        print(f\"Config: Embed: {embed_dim}, Window: {window}, Batch: {batch_size}, LR: {learning_rate}, Epochs: {num_epochs}\")\n",
    "                        ag.test_cbow_performance(cbow_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "d7vzldCV_Xhm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Skip Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qIcLx44H00tj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The Skip Gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UDGcOm6L_WxO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change\n",
    "SKIP_EMBED_DIMENSIONS = 100\n",
    "SKIP_WINDOW = 4\n",
    "SKIP_MAX_LENGTH = 50\n",
    "SKIP_BATCH_SIZE = 1024\n",
    "SKIP_NUM_EPOCHS = 2\n",
    "SKIP_LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "tc1pGyIN02q0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the Skip Gram model, we must prepare the data for training. The Skip Gram model learns to predict words to the left and right of a given word.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `x` is the index of a word in the corpus.\n",
    "* `y` is a list of indexes of words to the left of `x` or to the right of `x`.\n",
    "(Note the organization of the data is the opposite of the CBOW model)\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=brown`, `y=[the, quick, fox, jumped]`\n",
    "2. `x=fox`, `y=[quick, brown, jumped, over]`\n",
    "3. `x=jumped`, `y=[brown, fox, over, the]`\n",
    "4. `x=over`, `y=[fox, jumped, the, lazy]`\n",
    "5. `x=the`, `y=[jumped, over, lazy, dog]`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabular)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_skip_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_skip_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data, where each `y` is a list of word indices\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pKPaXDir-8ef",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "    data_out = []\n",
    "    vocab = Vocab()\n",
    "  ### BEGIN SOLUTION\n",
    "    for sentence in data_frame['text']:\n",
    "        # 使用分词器函数对句子进行分词\n",
    "        tokens = tokenizer_fn(sentence)\n",
    "\n",
    "        # 更新词汇表\n",
    "        for token in tokens:\n",
    "            vocab.add_word(token)\n",
    "        \n",
    "        if len(tokens)< ((window * 2) + 1):\n",
    "            continue\n",
    "\n",
    "        # 如果句子过长，只考虑前max_length个词\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "\n",
    "        # 遍历每个可能的窗口中心词\n",
    "        for i in range(window, len(tokens) - window):\n",
    "            # 收集上下文词的索引\n",
    "            context = [vocab.word2index(tokens[j]) for j in range(i-window, i+window+1) if j != i]\n",
    "\n",
    "            # 获取中心词的索引\n",
    "            target = vocab.word2index(tokens[i])\n",
    "\n",
    "            # 添加到输出数据中\n",
    "            data_out.append((target, context))\n",
    "    ### END SOLUTION\n",
    "    return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": false,
    "id": "HqoX7hkU_gf6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "SKIP_DATA, SKIP_VOCAB = prep_skip_gram_data(WIKI_TRAIN, my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": false,
    "id": "tsWgOrt-QVoB",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_DATA[0]\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8a8y6vLKDe1f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: compute the number of data points that should be in SKIP_DATA and check the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": false,
    "id": "YpNy1Qg1sK1u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected data points 625869\n",
      "actual data points 625869\n",
      "difference 0\n",
      "\n",
      "least vocab size 28782\n",
      "actual vocab size 28782\n",
      "\n",
      "Test passed!\n",
      "Test H: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - H (5 points)\n",
    "ag.check_data_size_h(WIKI_TRAIN, SKIP_WINDOW, SKIP_DATA, SKIP_VOCAB, max_length=SKIP_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "jYzbwH0BDui4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Skip Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "u03pc5UiD2db",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the Skip Gram model specification.\n",
    "\n",
    "The Skip Gram model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a single token index and produces a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "QmBQWNxS5zAd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # 定义嵌入层，它将索引转换为嵌入向量\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        # 定义线性层，它将嵌入向量转换为词汇表大小的输出，以预测上下文\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过嵌入层获取x的嵌入向量\n",
    "        embeds = self.embeddings(x)\n",
    "        # 将嵌入向量通过线性层\n",
    "        out = self.linear(embeds)\n",
    "        # 应用log softmax来获取概率分布\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqgDR_KbEGaf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: check the layers and layer ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "skip_model = SkipGram(SKIP_VOCAB.num_words(), SKIP_EMBED_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": false,
    "id": "s_Td11UJQIcv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has two layers as expected!\n",
      "Your layers orders are as expected!\n",
      "Test passed!\n",
      "Test I: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test I (5 points)\n",
    "ag.test_skipgram_structure(skip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UoCtW4HhDniu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": false,
    "id": "dfeBc-qu_MkK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_CRITERION = nn.NLLLoss()\n",
    "  SKIP_OPTIMIZER = torch.optim.AdamW(skip_model.parameters(), lr=SKIP_LEARNING_RATE)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": false,
    "id": "tpyRM2LjypCq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_skipgram(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = None\n",
    "      # Calculate loss for every word in the context\n",
    "      for word in y.T:\n",
    "        if loss is None:\n",
    "          loss = criterion(y_hat, word)\n",
    "        else:\n",
    "          loss += criterion(y_hat, word)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item() / y.shape[1])\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": false,
    "id": "RZOfUwgsEUQe",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 10.450851440429688\n",
      "iter 100 loss 9.557377154284184\n",
      "iter 200 loss 8.552803839024026\n",
      "iter 300 loss 7.660844748994441\n",
      "iter 400 loss 6.990698989192744\n",
      "iter 500 loss 6.499340323868863\n",
      "iter 600 loss 6.13663020744895\n",
      "epoch 0 loss 6.105807384765675\n",
      "iter 0 loss 4.983040809631348\n",
      "iter 100 loss 4.268890052738756\n",
      "iter 200 loss 4.003258628038624\n",
      "iter 300 loss 3.8760668003677927\n",
      "iter 400 loss 3.8004145099040576\n",
      "iter 500 loss 3.7504902043028507\n",
      "iter 600 loss 3.725188686129654\n",
      "epoch 1 loss 3.724487073682919\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  train_skipgram(skip_model, SKIP_DATA, num_epochs=SKIP_NUM_EPOCHS, batch_size=SKIP_BATCH_SIZE, criterion=SKIP_CRITERION, optimizer=SKIP_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VdEsOoV4PlTh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we have trained the Skipgram model, we will be using the `WIKI_TEST` dataset again for evaluation. Your Skipgram model will need to achieve at least 30% accuracy to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": false,
    "id": "tmNDlKPRNR1G",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "        token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "    for i in range(window, len(token_ids)-window):\n",
    "      x = token_ids[i]\n",
    "      y = token_ids[i-window:i]\n",
    "      y.extend(token_ids[i+1:i+1+window])\n",
    "      data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_skip_gram_test_data(WIKI_TEST, SKIP_VOCAB, tokenizer_fn=my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": false,
    "id": "fmVj4r1EtEr7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed! Accuracy = 0.2414517337328767/1\n",
      "Test J: 0/20\n"
     ]
    }
   ],
   "source": [
    "# student check - Test J (20 points)\n",
    "ag.test_skip_performance(skip_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change\n",
    "SKIP_EMBED_DIMENSIONS = 100\n",
    "SKIP_WINDOW = 4\n",
    "SKIP_MAX_LENGTH = 50\n",
    "SKIP_BATCH_SIZE = 1024\n",
    "SKIP_NUM_EPOCHS = 3\n",
    "SKIP_LEARNING_RATE = 5e-4\n",
    "\n",
    "skip_model = SkipGram(SKIP_VOCAB.num_words(), SKIP_EMBED_DIMENSIONS)\n",
    "SKIP_CRITERION = nn.NLLLoss()\n",
    "SKIP_OPTIMIZER = torch.optim.AdamW(skip_model.parameters(), lr=SKIP_LEARNING_RATE)\n",
    "\n",
    "train_skipgram(skip_model, SKIP_DATA, num_epochs=SKIP_NUM_EPOCHS, batch_size=SKIP_BATCH_SIZE, criterion=SKIP_CRITERION, optimizer=SKIP_OPTIMIZER)\n",
    "\n",
    "TEST_DATA = prep_skip_gram_test_data(WIKI_TEST, SKIP_VOCAB, tokenizer_fn=my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)\n",
    "\n",
    "ag.test_skip_performance(skip_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "Please submit this .ipynb file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your projected points for this assignment is 60/100.\n",
      "\n",
      "NOTE: THIS IS NOT YOUR FINAL GRADE. YOUR FINAL GRADE FOR THIS ASSIGNMENT WILL BE AT LEAST 60 OR MORE, BUT NOT LESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time in minutes = 46.04129377603531\n",
      "WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\n"
     ]
    }
   ],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
